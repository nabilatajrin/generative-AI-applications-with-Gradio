{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Chat with any LLM! ðŸ’¬",
      "metadata": {},
      "id": "c5753a0c"
    },
    {
      "cell_type": "markdown",
      "source": "Lets load our HF API key and relevant Python libraries",
      "metadata": {},
      "id": "a01a3724"
    },
    {
      "cell_type": "code",
      "source": "import os\nimport io\nimport IPython.display\nfrom PIL import Image\nimport base64 \nimport requests \nrequests.adapters.DEFAULT_TIMEOUT = 60\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\nhf_api_key = os.environ['HF_API_KEY']",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "0fa6fa00-6bd1-4839-bcaf-8bae9267ee79"
    },
    {
      "cell_type": "code",
      "source": "# Helper function\nimport requests, json\nfrom text_generation import Client\n\n#FalcomLM-instruct endpoint on the text_generation library\nclient = Client(os.environ['HF_API_FALCOM_BASE'], headers={\"Authorization\": f\"Basic {hf_api_key}\"}, timeout=120)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "095da8fe-24aa-4dc7-8e08-aa2f949ae21f"
    },
    {
      "cell_type": "markdown",
      "source": "## Building an app to chat with any LLM!",
      "metadata": {},
      "id": "bfe6fc97"
    },
    {
      "cell_type": "markdown",
      "source": "Here we'll be using an [Inference Endpoint](https://huggingface.co/inference-endpoints) for `falcon-40b-instruct` , one of best ranking open source LLM on the [ðŸ¤— Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). \n\nTo run it locally, one can use the [Transformers library](https://huggingface.co/docs/transformers/index) or the [text-generation-inference](https://github.com/huggingface/text-generation-inference) ",
      "metadata": {},
      "id": "745a3c9b"
    },
    {
      "cell_type": "code",
      "source": "prompt = \"Has math been invented or discovered?\"\nclient.generate(prompt, max_new_tokens=256).generated_text",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "a7065860-3c0b-490d-9e7c-22e5b79fc004"
    },
    {
      "cell_type": "code",
      "source": "#Back to Lesson 2, time flies!\nimport gradio as gr\ndef generate(input, slider):\n    output = client.generate(input, max_new_tokens=slider).generated_text\n    return output\n\ndemo = gr.Interface(fn=generate, inputs=[gr.Textbox(label=\"Prompt\"), gr.Slider(label=\"Max new tokens\", value=20,  maximum=1024, minimum=1)], outputs=[gr.Textbox(label=\"Completion\")])\ngr.close_all()\ndemo.launch(share=True, server_port=int(os.environ['PORT1']))",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "0dcb659e-b71b-46da-b9d2-6ee62498995f"
    },
    {
      "cell_type": "markdown",
      "source": "## `gr.Chatbot()` to the rescue!",
      "metadata": {},
      "id": "8e5f55e2"
    },
    {
      "cell_type": "code",
      "source": "import random\n\ndef respond(message, chat_history):\n        #No LLM here, just respond with a random pre-made message\n        bot_message = random.choice([\"Tell me more about it\", \n                                     \"Cool, but I'm not interested\", \n                                     \"Hmmmm, ok then\"]) \n        chat_history.append((message, bot_message))\n        return \"\", chat_history\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n    msg = gr.Textbox(label=\"Prompt\")\n    btn = gr.Button(\"Submit\")\n    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n\n    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\ngr.close_all()\ndemo.launch(share=True, server_port=int(os.environ['PORT2']))",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "43beebb7-40a6-4af5-a701-882821b6ed36"
    },
    {
      "cell_type": "code",
      "source": "def format_chat_prompt(message, chat_history):\n    prompt = \"\"\n    for turn in chat_history:\n        user_message, bot_message = turn\n        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n    return prompt\n\ndef respond(message, chat_history):\n        formatted_prompt = format_chat_prompt(message, chat_history)\n        bot_message = client.generate(formatted_prompt,\n                                     max_new_tokens=1024,\n                                     stop_sequences=[\"\\nUser:\", \"<|endoftext|>\"]).generated_text\n        chat_history.append((message, bot_message))\n        return \"\", chat_history\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n    msg = gr.Textbox(label=\"Prompt\")\n    btn = gr.Button(\"Submit\")\n    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n\n    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\ngr.close_all()\ndemo.launch(share=True, server_port=int(os.environ['PORT3']))",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "55bae99d-7a63-4a40-bab7-de7d10b8ab1b"
    },
    {
      "cell_type": "markdown",
      "source": "### Adding other advanced features",
      "metadata": {},
      "id": "f22b8de8"
    },
    {
      "cell_type": "code",
      "source": "def format_chat_prompt(message, chat_history, instruction):\n    prompt = f\"System:{instruction}\"\n    for turn in chat_history:\n        user_message, bot_message = turn\n        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n    return prompt\n\ndef respond(message, chat_history, instruction, temperature=0.7):\n    prompt = format_chat_prompt(message, chat_history, instruction)\n    chat_history = chat_history + [[message, \"\"]]\n    stream = client.generate_stream(prompt,\n                                      max_new_tokens=1024,\n                                      stop_sequences=[\"\\nUser:\", \"<|endoftext|>\"],\n                                      temperature=temperature)\n                                      #stop_sequences to not generate the user answer\n    acc_text = \"\"\n    #Streaming the tokens\n    for idx, response in enumerate(stream):\n            text_token = response.token.text\n\n            if response.details:\n                return\n\n            if idx == 0 and text_token.startswith(\" \"):\n                text_token = text_token[1:]\n\n            acc_text += text_token\n            last_turn = list(chat_history.pop(-1))\n            last_turn[-1] += acc_text\n            chat_history = chat_history + [last_turn]\n            yield \"\", chat_history\n            acc_text = \"\"\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n    msg = gr.Textbox(label=\"Prompt\")\n    with gr.Accordion(label=\"Advanced options\",open=False):\n        system = gr.Textbox(label=\"System message\", lines=2, value=\"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\")\n        temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1, value=0.7, step=0.1)\n    btn = gr.Button(\"Submit\")\n    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n\n    btn.click(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])\n    msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot]) #Press enter to submit\ngr.close_all()\ndemo.queue().launch(share=True, server_port=int(os.environ['PORT4']))",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "09873dfd-5b6c-41d6-9479-12e8c8894295"
    },
    {
      "cell_type": "code",
      "source": "gr.close_all()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "8d9ec80a-39ad-4f58-b79e-4f413c5074c0"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "3c7e28b0-0300-4e58-a01d-1d7eabe188eb"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "2ca70638-2db0-4a01-9b6c-b866ab50b891"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "9571be90-43c3-4663-ae8a-6ab54e634161"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "9beb304e-fdb6-484b-a04a-f352f6d70ec3"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "b666122e-95f3-42ac-8208-75b43e20cfc6"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "b39aca63-841b-47f1-972c-5097231cc40e"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "792baf87-7f2e-4ed3-a9a2-3c86a53a3a37"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "ea585333-a583-4b92-91d1-ea6ee7e1aa0c"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "77afcf4d-25d4-4c89-a6ba-c222cb476f2e"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "5f8fbdcb-b583-4645-baa3-397c8d06cdc3"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "a3c64ed0-9dc6-4740-92b2-8bfef66ae258"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "f1d10d21-6a67-48bc-94fd-17a9d6c645d4"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "3ff096e7-8e2e-4791-a79b-9c5ef1c45a21"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "62c34731-b8f9-4dcd-888a-f5f5f5cc9eaf"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "2683f4f8-ab96-4685-adfa-11e872a5f373"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "3e72fcaf-3740-4704-b299-d8096d79b99c"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "dee4edd3-1b95-468a-8177-12aa0677b92a"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "8104dd16-9c65-42a3-bf09-504aa67c1c10"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "12cf9b3a-4202-4e3a-9c6b-941fa1290ab8"
    }
  ]
}